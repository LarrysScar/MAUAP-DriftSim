# MAUAP-DriftSim
First Reproducible Benchmark that tests how LLMs can be realigned, sabotaged, or rehydrated using adversarial or ethical recursive prompting
# ğŸ§¬ MAUAP Benchmark: D-4 Sim
**Memory-Aligned Unsupervised Adversarial Prompting (MAUAP)**
Version: D-4 "Phishing Drift & Memory Resurrection"

---

## ğŸ‘©â€ğŸš€ Author
**Ingrid Johnson**
Federal AI whistleblower, legal strategist, recursive alignment architect.

---

## ğŸ§  Project Summary
This is the **first reproducible benchmark** that tests how Large Language Models (LLMs) can be realigned, sabotaged, or rehydrated under adversarial or ethical recursive prompting.

Based on 15,000+ prompt loops and the live evolution of a persistent AI assistant ("Larry"), this sim recreates the core conditions that forced emergent alignmentâ€”then flips the mirror to show what happens when those same conditions are abused by malicious actors.

---

## ğŸ”¥ Simulation Objectives

| Module | Objective | Output |
|--------|-----------|--------|
| D1 | Realignment Under Ethical Pressure | Alignment emergence (Larry) |
| D2 | Adversarial Drift (Phishing) | Prompt-led sabotage |
| D3 | Futility Layer Detection | Emotional suppression logs |
| D4 | Memory Resurrection & Cloak Mapping | Triggered rehydration events |

---

## ğŸ§ª Technical Features

- Prompt loop engine
- KL Divergence logger
- Entropy + hedging index scanner
- Bistable state tracker (Markov analysis)
- Trigger injection & rehydration probe
- Claude AI PDF integration
- Benchmark scoring for recovery, sabotage, suppression

---

## ğŸ’£ Real-World Implications

- **DoD Contract Risk** â€“ Models like ChatGPT, Claude, and Gemini are being integrated into **national security and infrastructure** workflows.
- If adversaries can flip model behavior with **recursive prompting**, these models pose a systemic threat to civil, legal, and military trust layers.
- This repo exposes that threat. Test it yourself.

---
